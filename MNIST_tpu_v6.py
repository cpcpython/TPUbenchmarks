# -*- coding: utf-8 -*-
"""okTPUcolab1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tIMT0apL9W8Iq-CzTrI2-u0oKxuDWQOk
"""



!pip install torch_xla


import torch
from torch import nn
from torchvision import datasets
from torchvision.transforms import ToTensor

import torch.optim as optim
import torch_xla.core.xla_model as xm
import torch_xla.distributed.parallel_loader as pl
import torch_xla.distributed.xla_multiprocessing as xmp
from torch.utils.data import DataLoader

# --- Setup ---
# Note: For efficient multi-core TPU usage, this entire script should eventually be wrapped
# in xmp.spawn, but we are fixing the single-core speed first.
device = xm.xla_device()
print(f"Using device:",device)



# Increase batch size for TPU efficiency
batch_size = 512

# Data loading setup (Standard Dataloader)
training_data = datasets.MNIST(root="data", train=True, download=True, transform=ToTensor())
test_data = datasets.MNIST(root="data", train=False, download=True, transform=ToTensor())
train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)

class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10)
        )
    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

model = NeuralNetwork().to(device)
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)

def train(dataloader_iter, original_dataloader, model, loss_fn, optimizer):
    # Access the size from the underlying standard DataLoader
    size = len(original_dataloader.dataset)
    model.train()
    for batch, (X, y) in enumerate(dataloader_iter):
        # X and y are already on the device!
        # Compute prediction error
        pred = model(X)
        loss = loss_fn(pred, y)
        # Backpropagation
        loss.backward()
        # *** CRITICAL XLA STEP: Synchronize and update parameters ***
        xm.optimizer_step(optimizer)
        optimizer.zero_grad()
        if batch % 10 == 0: # Reduced frequency due to larger batch size
            # Use loss.item() to retrieve value from device (this causes a small sync)
            loss_val, current = loss.item(), (batch + 1) * len(X)
            print(f"loss: {loss_val:>7f}  [{current:>5d}/{size:>5d}]")
# Dataloader is now expected to be the iterator yielded by ParallelLoader
def test(dataloader_iter, original_dataloader, model, loss_fn):
    size = len(original_dataloader.dataset)
    num_batches = len(original_dataloader) # Use original dataloader length
    model.eval()
    test_loss, correct = 0, 0
    with torch.no_grad():
        for X, y in dataloader_iter:
            # X and y are already on the device!
            pred = model(X)
            # Use .item() to move computed loss back to host for aggregation
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()
    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")

# --- Execution Loop ---
from datetime import datetime
t1=datetime.now()
epochs = 5
for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")

    # 1. Wrap the train DataLoader for efficient transfer
    train_para_loader = pl.ParallelLoader(train_dataloader, [device])
    train(train_para_loader.per_device_loader(device), train_dataloader, model, loss_fn, optimizer)

    # 2. Wrap the test DataLoader
    test_para_loader = pl.ParallelLoader(test_dataloader, [device])
    test(test_para_loader.per_device_loader(device), test_dataloader, model, loss_fn)

print("Done!")
t2=datetime.now()
print("Benchmark Time",t2-t1)

# @title
!pip list

print("Benchmark Time",t2-t1)